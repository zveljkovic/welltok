Id,title,description,author,tags,created_at,updated_at
1,Brazil opens vast Amazon reserve to mining,"Brazil's government has abolished a vast national reserve in the Amazon to open up the area to mining.
The area, covering 46,000 sq km (17,800 sq miles), straddles the northern states of Amapa and Para, and is thought to be rich in gold, and other minerals.
The government said nine conservation and indigenous land areas within it would continue to be legally protected.
But activists have voiced concern that these areas could be badly compromised.
A decree from President Michel Temer abolished a protected area known as the National Reserve of Copper and Associates (Renca).
Its size is larger than Denmark and about 30% of it will be open to mining.
The mining and energy ministry says protected forest areas and indigenous reserves will not be affected.
""The objective of the measure is to attract new investments, generating wealth for the country and employment and income for society, always based on the precepts of sustainability,"" the ministry said in a statement.
",BBC,"Brazil, amazon",4.12.2016,4.12.2017
2,Google will ask: 'Are you depressed?',"People searching for “depression” on Google will soon be prompted to take a questionnaire to assess if they may be suffering from the illness.
The search giant has partnered with the US National Alliance on Mental Illness (Nami) to roll out the project which is currently only for US users.
Users searching for depression will be prompted to “check if you’re clinically depressed”.
“While this tool can help, it’s important to note that PHQ-9 is not meant to act as a singular tool for diagnosis,” Nami said.
In a blog post announcing the news, Nami said the test should not be seen as replacing the insight of qualified mental health professionals - was instead a method to help people get the right help more quickly.
""By tapping “Check if you’re clinically depressed,” you can take this private self-assessment to help determine your level of depression and the need for an in-person evaluation,” the organisation explained.
""The results of the PHQ-9 can help you have a more informed conversation with your doctor.""
'Trouble concentrating?'
The question will appear in the Knowledge Panel - the box that appears at the top of results when users search on a mobile device. Typically this panel is used for factual information, including details drawn from Wikipedia entries.
The Patient Health Questionnaire-9 is a series of nine questions about the subject’s mental health.
It asks how often you feel you have “little interest or pleasure in doing things” or “trouble concentrating on things, such as reading the newspaper or watching television?”.
Various studies have concluded it is a concise, reliable way to accurately detect signs of clinical depression.
Speaking to the Financial Times, Google product manager Vidushi Tekriwal said users who fill out the test will not have their answers logged by the company, nor would advertising be targeted to them as a result.
However, one psychotherapist said the idea seemed ""terribly redundant"".
Someone googling depression will probably not find more useful information via a short diagnostic than they have already surfaced in search results, argued Dr Aaron Balick, author of ""The Psychodynamics of Social Netowrking"".
""A better approach would be some sort of acknowledgment that the searcher may be feeling down, and offering them resources and a direct line - perhaps a chat box - to local psychological services,"" he told the BBC.
",Dave Lee,"Google, depression",11.15.2016,11.18.2016
3,Wayne Rooney international retirement: How will history judge his England career?,"Wayne Rooney's retirement from international duty - announced after England's all-time record goalscorer had been told he had won a recall at 31 - closes another chapter in his illustrious story.
Rooney, with 53 England goals in 119 appearances, left Manchester United to return to Everton this summer with his place in Old Trafford's history books also assured after he overtook Sir Bobby Charlton's club record with 253 goals.
He had been sidelined by Jose Mourinho at United, and also by Southgate with England, but such has been his early rejuvenation at Goodison Park that he was offered a place back in the fold for the forthcoming World Cup qualifiers against Malta and Slovakia.
Rooney, who originally planned to end his England career after next summer's World Cup in Russia, decided against a return and will now focus fully on Everton.
So how will history judge Wayne Rooney's England career?
",BBC Sport,"Rooney, England, Soccer",4.12.2012,6.19.2017
4,US Open: Milos Raonic withdraws because of wrist injury,"World number 11 Milos Raonic is the latest high-profile player to pull out of the US Open, which starts on Monday.
Canadian Raonic, 26, has withdrawn because of a persistent wrist injury but said that he planned to return to action before the end of 2017.
Defending champion Stan Wawrinka is unable to defend his singles title at Flushing Meadows because of a knee injury.
Novak Djokovic (elbow) and Victoria Azarenka will also be absent.
Belarusian Azarenka, 28, is missing the final Grand Slam of the year because of an ""ongoing family situation"".
World number 10 Kei Nishikori is also out after tearing a tendon in his right wrist.
Raonic said he was unable give ""full effort"" as a result of his injury.
""I have too much respect for the US Open and my fellow competitors to take a spot in the draw when I know I cannot give full effort due to this injury,"" he said.
",BBB Sports,"US Open, Tennis, Raonic",3.2.2015,3.5.2015
5,"Billionaires and big ag are joining venture investors to fund lab-grown meat
","Eighty-five years ago, Winston Churchill wrote an article for Popular Mechanics that predicted humans would soon be growing their meat rather than cultivating animals for it.
Now, with $17 million in fresh financing from a slew of new investors, including the billionaires Bill Gates and Richard Branson, the big agriculture company Cargill and the venture capital firm DFJ, Memphis Meats is hoping to create an entirely new industry around what it calls “clean meat.”
“Instead of using animals as pieces of technology to convert plants into proteins to make things that we like to eat, drink and wear, we can just use biology to make those things directly,” said Seth Bannon, a co-founder of the upstart venture firm Fifty Years and an early investor in Memphis Meats.
The company has already successfully made synthesized beef, chicken and duck, according to Memphis Meats co-founder and chief executive Uma Valeti. Now the trick is to get the company to grow their meat at scale.
“We envision this to be a production facility where people can walk through and see where the meat is growing, where it is being harvested and where it is being cooked. You don’t get to visit feed lots or visit slaughterhouses,” Valeti tells me.
Valeti imagines a production facility that looks more like a craft brewery than a slaughterhouse. It also would represent the first major innovation in the meat industry in the 10,000 years since humans first began breeding livestock.
In a 2002 article for The New York Times Magazine, journalist Michael Pollan described how cows are slaughtered. 
The cows are funneled into a chute single-file. Once there, they are walked over a metal bar, and, as the floor declines, the cows are suspended over a false floor on the bar and then taken on a conveyor belt to pass in front of a slaughterhouse employee called a “stunner.”
The stunner’s job is to shoot a seven-inch steel bolt, roughly the width of a pencil, between the eyes of the drugged and incapacitated cow.
Then the dead animal is moved from the conveyor belt to a trolley overhead and carried to the bleeding area, where its throat is cut. Roughly 392 cows are slaughtered per hour at a typical slaughterhouse (like the one in Kansas that Pollan described).
This is the culmination of human achievement in meat processing so far (don’t even get me started on chickens).
",TechCrunch,"Billionaires, Ventures",8.7.2012,8.7.2013
6,"Thanks to Amazon, Seattle is now America’s biggest company town
","Amazon’s extraordinary growth has turned Seattle into the biggest company town in America.
Amazon now occupies a mind-boggling 19 percent of all prime office space in the city, the most for any employer in a major U.S. city, according to a new analysis conducted for The Seattle Times.

Amazon’s footprint in Seattle is more than twice as large as any other company in any other big U.S. city, and the e-commerce giant’s expansion here is just getting started.

The swarms of 20-somethings crowding into South Lake Union every morning represent an urban campus that is unparalleled in the United States — and they have helped transform Seattle, for better or worse. Amazon’s rapid rise has fueled an economy that has driven up wages and lowered unemployment, but also produced gridlock on the roads and sky-high housing prices.
And while Seattle’s booming economy is often attributed to a wide variety of factors, increasingly, it’s all about one company.
Amazon now occupies more office space than the next 40 biggest employers in the city combined.
And that’s only the beginning: Amazon’s Seattle footprint of 8.1 million square feet is expected to soar to more than 12 million square feet within five years.
",Amazon,"Amazon, Seattle",3.6.2011,3.9.2016
7,This is How Google will Collapse,"Google made almost all its money from ads. It was a booming business — until it wasn’t. Here’s how things looked right before the most spectacular crash the technology industry had ever seen.
The crumbling of Google’s cornerstone
Search was Google’s only unambiguous win, as well as its primary source of revenue, so when Amazon rapidly surpassed Google as the top product search destination, Google’s foundations began to falter. As many noted at the time, the online advertising industry experienced a major shift from search to discovery in the mid-2010s.
While Google protected its monopoly on the dying search advertising market, Facebook — Google’s biggest competitor in the online advertising space — got on the right side of the trend and dominated online advertising with its in-feed native display advertising.",Hackernoon,"Google, collapse",7.2.2012,6.8.2014
8,"Universities are broke. So let’s cut the pointless admin and get back to teaching
","A
s students have been celebrating their exam results, pundits from across the political spectrum have been commiserating the state of British universities. Andrew Adonis, an education minister during the Blair years, has excoriated universities for offering costly courses while jacking up the pay of their senior leaders. Nick Timothy, Theresa May’s ex-advisor, thinks UK universities are an unsustainable “Ponzi scheme”. The universities minister, Jo Johnson, has written about the need to put further pressure on seats of higher learning so students get good value for money.
Behind the political point-scoring are more serious issues. The university sector has been growing for decades, but now that growth is going into reverse. The number of undergraduates applying to universities has fallen by 4% this year. Although close to 50% of the population goes through higher education, only about 20% of jobs require an undergraduate degree. One US study found that 46% of students showed no improvement in their cognitive skills during their time at university. In some courses, like business administration, students’ capacity to think got worse for the first few years. And after they graduated, many struggled to find full-time work while being loaded down with debt. Nearly a quarter of graduates were living with their parents or relatives.
",Guardian,Universities,5.12.2015,5.14.2015
9,"New and improved bike routing, with low stress options
","Bicyclists ride to commute, to exercise, for sport, for leisure. But no matter the reason, most cyclists will ride on the road at some point, and when they do, they’ll think about the safety and comfort of their route.
Should you take a longer route to ride on a road with a dedicated cycling lane? Do you take a left turn to avoid those difficult hills? Different cyclists will make different choices, as the ideal route for one bicyclist may not be so ideal for another.
Valhalla, the open-source routing engine that powers Mapzen Turn-by-Turn, has always had customization parameters. While these include use_roads and use_hills, setting these to zero didn’t quite define what most would consider a “low stress” bike route. At least not until now!
This summer, I and the Mapzen routing team enhanced Valhalla’s bicycle costing system. Not only is overall routing improved, but use_roads and use_hillsare now much more versatile: when both of these parameters are set to 0, Valhalla generates a nice, “low-stress” biking route. As these parameters get closer to 1, it linearly increases to a more “professional” biking route.
Along with this new update, our default bike settings have also changed — use_roads and use_hills previously defaulted to 0.5, but now they default to 0.25. The default bike (and thus speed) has also been changed from a road bike to a hybrid bike, which is more suited for city riding.
Technical Details
The meat of this update is the added support for low-stress biking. Our original goal was to introduce a new “low-stress bike” costing class derived from the regular bike costing class. However, after doing this, we realized that the algorithms of the new LowStressBicycleCost class were actually cleaner and easier to manage than our original bike costing algorithms. This inspried us to rewrite the old bike code to look like the new low-stress bike code. This way, the use_roads and use_hills parameters acted as variables that scale the costs of edges (i.e. road segments) with particular attributes in a way that provides low-stress biking at one end of the spectrum, and professional road biking on the other.
When originally making the LowStressBicycleCost class, we made use of research by Michael B. Lowry at the University of Idaho, Peter Furth at Northeastern, and Tracy Hadden-Loh with the Rails-to-Trails Conservancy. Their paper titled “Low-Stress Neighborhood Bikeability Assessment to Prioritize Bicycle Infrastructure” provides a way to score a road based on how stressful it is to bike on. Here are the variables that are included in the score:
",Mapzen,Bikes,5.12.2015,12.12.2016
10,D as a Better C,"D was designed from the ground up to interface directly and easily to C, and to a lesser extent C++. This provides access to endless C libraries, the Standard C runtime library, and of course the operating system APIs, which are usually C APIs.
But there’s much more to C than that. There are large and immensely useful programs written in C, such as the Linux operating system and a very large chunk of the programs written for it. While D programs can interface with C libraries, the reverse isn’t true. C programs cannot interface with D ones. It’s not possible (at least not without considerable effort) to compile a couple of D files and link them in to a C program. The trouble is that compiled D files refer to things that only exist in the D runtime library, and linking that in (it’s a bit large) tends to be impractical.
D code also can’t exist in a program unless D controls the main() function, which is how the startup code in the D runtime library is managed. Hence D libraries remain inaccessible to C programs, and chimera programs (a mix of C and D) are not practical. One cannot pragmatically “try out” D by add D modules to an existing C program.
That is, until Better C came along.
It’s been done before, it’s an old idea. Bjarne Stroustrup wrote a paper in 1988 entitled “A Better C“. His early C++ compiler was able to compile C code pretty much unchanged, and then one could start using C++ features here and there as they made sense, all without disturbing the existing investment in C. This was a brilliant strategy, and drove the early success of C++.
A more modern example is Kotlin, which uses a different method. Kotlin syntax is not compatible with Java, but it is fully interoperable with Java, relies on the existing Java libraries, and allows a gradual migration of Java code to Kotlin. Kotlin is indeed a “Better Java”, and this shows in its success.
D as Better C
D takes a radically different approach to making a better C. It is not an extension of C, it is not a superset of C, and does not bring along C’s longstanding issues (such as the preprocessor, array overflows, etc.). D’s solution is to subset the D language, removing or altering features that require the D startup code and runtime library. This is, simply, the charter of the -betterC compiler switch.
Doesn’t removing things from D make it no longer D? That’s a hard question to answer, and it’s really a matter of individual preference. The vast bulk of the core language remains. Certainly the D characteristics that are analogous to C remain. The result is a language somewhere in between C and D, but that is fully upward compatible with D.
Removed Things
Most obviously, the garbage collector is removed, along with the features that depend on the garbage collector. Memory can still be allocated the same way as in C – using malloc() or some custom allocator.
Although C++ classes and COM classes will still work, D polymorphic classes will not, as they rely on the garbage collector.
Exceptions, typeid, static construction/destruction, RAII, and unittests are removed. But it is possible we can find ways to add them back in.
Asserts are altered to call the C runtime library assert fail functions rather than the D runtime library ones.
(This isn’t a complete list, for that see http://dlang.org/dmd-windows.html#switch-betterC.)
Retained Things
More importantly, what remains?
What may be initially most important to C programmers is memory safety in the form of array overflow checking, no more stray pointers into expired stack frames, and guaranteed initialization of locals. This is followed by what is expected in a modern language — modules, function overloading, constructors, member functions, Unicode, nested functions, dynamic closures, Compile Time Function Execution, automated documentation generation, highly advanced metaprogramming, and Design by Introspection.
",D Lang,"D, C, languages",10.10.2010,11.11.2011
11,"Why I haven't jumped ship from Common Lisp to Racket (just yet)
","Matthias Felleisen jested ""Why are you still using CL when Scrbl/Racket is so much better :-)"" ? My response was as follows:
Dear Matthias,
you are right Racket is so much better in so many dimensions. I use Lisp because I just can't bear programming in a language without proper syntactic abstraction, and that is a dimension where Racket is far ahead of Common Lisp (CL), which sadly also remains far ahead of the rest of competition. Racket also has managed to grow a remarkable way to mix typed and untyped program fragments, which sets it ahead of most. But I am under the impression that there are still many dimensions in which Racket lags behind other languages and Common Lisp (CL) in particular.
	1.	The Common Lisp Object System (CLOS) has multiple-inheritance, multi-methods, method combinations, introspection and extensibility via the MOP, generic functions that work on builtin classes, support for dynamic instance class change (change-class, update-instance-for-changed-class) and class redefinition (defclass, update-instance-for-redefined-class), a semi-decent story for combining parametric polymorphism and ad hoc polymorphism (my own lisp-interface-library), etc. Racket seems to still be playing catch-up with respect to ad hoc polymorphism, and is lacking a set of good data structure libraries that take advantage of both functional and object-oriented programming (a good target is Scala's scalaz or its rival cats).
	2.	While the ubiquity of global side-effects in CL is very bad, the facts that all objects that matter are addressable by a path from some global namespace and that live redefinition is actively supported makes debugging and maintaining long-lived systems with in-image persistent data more doable (see again CLOS's update-instance-for-redefined-class). This is in contrast with the Racket IDE which drops live data when you recompile the code, which is fine for student exercises, but probably wrong for live systems. CL is one of the few languages that takes long-term data seriously (though not quite as seriously as Erlang).
	3.	Libraries. CL seems to have much more libraries than Racket, and though the quality varies, these libraries seem to often have more feature coverage and more focus on production quality. From a cursory look, Racket libraries seem to often stop at ""good enough for demo"". An effort on curating libraries, homogenizing namespaces, etc., could also help Racket (I consider CL rather bad in this respect, yet Racket seems worse). My recent experience with acmart, my first maintained Racket library, makes me think that writing libraries is even higher overhead in Racket than in CL, which is already mediocre.
	4.	Speedwise, SBCL still produces code that runs noticeably faster than Racket (as long as you don't need full delimited control, which would requires a much slower CL-to-CL compiler like hu.dwim.delico). This difference may be reduced as Racket adopts the notoriously fast Chez Scheme as a backend (or not). Actually, the announcement of the new Racket backend really makes me eager to jump ship.
	5.	As for startup latency, Common Lisp is also pretty good with its saved images (they start in tens of milliseconds on my laptop), making it practical to write trivial utilities for interactive use from the shell command-line with an ""instantaneous"" feel. Racket takes hundreds of milliseconds at startup which puts it (barely) in the ""noticeable delay"" category (though nowhere near as bad as anything JVM-based).
All these reasons, in addition to inertia (and a non-negligible code-base and mind-base), have made me stick to CL — for now. I think Racket is the future of Lisp (at least for me), I just haven't jumped ship right yet. If and when I do, I'll probably be working on some of these issues.
PS (still 2017-03): Here are ways that Racket is indeed vastly superior to CL, that make me believe it's the future of Lisp:
	•	First and foremost, Racket keeps evolving, and not just ""above"" the base language, but importantly below. This alone makes it vastly superior to CL (that has evolved tremendously ""above"" its base abstractions, but hasn't evolved ""below"", except for FFI purpose, in the last 20 years), which itself remains superior to most languages (that tend to not evolve much ""above"", and not at all ""below"" their base abstractions).
	•	Racket is by far ahead of the pack in terms of Syntactic abstraction. It is the best language in which to define other languages and experiment with them, bar none.
	•	Racket has a decent module system, including build and phase separation (even separate phases for testing, cross-compilation or whatever you want), and symbol selection and renaming.
	•	Racket has typed modules, and a good interface between typed and untyped modules. While types in Racket do not compete with those of say Haskell, just yet, they are still evolving, fast, and that contract interface between typed and untyped is far ahead of anything the competition has.
	•	Racket has lots of great teaching material.
	•	Racket has a one-stop-shop for documentation, though it isn't always easy to navigate and often lack examples. That still puts it far ahead of CL and a lot of languages.
	•	Racket provides purity by default a decent set of data structures.
	•	Racket has many primitives for concurrency, virtualization, sandboxing.
	•	Racket has standard primitives for laziness, pattern-matching, etc.
	•	Racket has a standard, portable, gui.
	•	Racket has a lively, healthy, user and developer community.
",Live Journal,"Lisp, Racket",6.20.2016.,12.20.2016.
12,"Exxon Duped Public Over Climate Concerns, Harvard Research Says
","Exxon Mobil Corp. spent the last 40 years undermining public concern over climate change, even as its own scientists determined man-made global warming was real and a serious threat, according to Harvard University researchers writing in a peer-reviewed journal.
“Exxon Mobil contributed to advancing climate science -- by way of its scientists’ academic publications -- but promoted doubt about it in advertorials,” the Harvard researchers wrote in the journal Environmental Research Letters. “Given this discrepancy, we conclude that Exxon Mobil misled the public.”
The findings could add fuel to lawsuits brought against the world’s largest oil explorer by market value. New York’s attorney general is probing whether Exxon lied to investors and the public for almost four decades about the impact of climate change on profits. Exxon is one of the world’s largest sources of fuels responsible for climate change, producing 10 million gallons of gasoline and other fuels every hour of every day.
The researchers said Exxon has disagreed with their conclusion and said its statements on public policy and climate science “have always reflected the global understanding of the issue,” according to an opinion piece written by two of the authors and published Wednesday in the New York Times.
Exxon said it acknowledges climate change is a risk that requires action, and it dismissed the conclusions of the study, saying the researchers are looking for money.
“The study was paid for, written and published by activists leading a five-year campaign against the company,” Exxon said in an emailed statement. “It is inaccurate and preposterous. Rather than pursuing solutions to address the risk of climate change, these activists, along with trial lawyers, have acknowledged a goal of extracting money from our shareholders and attacking the company’s reputation.”
Stranded Assets
The study’s authors, Geoffrey Supran and Naomi Oreskes, both scholars of scientific history at Harvard in Cambridge, Massachusetts, reviewed 187 climate change communications issued by Exxon between 1977 and 2014. Their article, “Assessing Exxon Mobil’s climate change communications,” was published Wednesday.
While 83 percent of Exxon’s peer-reviewed scientific papers and 80 percent of its internal documents acknowledge climate change is real and human-caused, 81 percent of its advertorials expressed doubt over the issue, according to the research. Internal documents accepted the risk of stranded assets caused by climate change, while the advertorials did not.
",Bloomberg,Exxon,3-3-13,4-4-14
13,"Mathematicians Tame Rogue Waves, Illuminating Future of LED Lighting
","In the 1950s, Philip Anderson, a physicist at Bell Laboratories, discovered a strange phenomenon. In some situations where it seems as though waves should advance freely, they just stop — like a tsunami halting in the middle of the ocean.
Anderson won the 1977 Nobel Prize in physics for his discovery of what is now called Anderson localization, a term that refers to waves that stay in some “local” region rather than propagating the way you’d expect. He studied the phenomenon in the context of electrons moving through impure materials (electrons behave as both particles and waves), but under certain circumstances it can happen with other types of waves as well.
Even after Anderson’s discovery, much about localization remained mysterious. Although researchers were able to prove that localization does indeed occur, they had a very limited ability to predict when and where it might happen. It was as if you were standing on one side of a room, expecting a sound wave to reach your ear, but it never did. Even if, after Anderson, you knew that the reason it didn’t was that it had localized somewhere on its way, you’d still like to figure out exactly where it had gone. And for decades, that’s what mathematicians and physicists struggled to explain.
This is where Svitlana Mayboroda comes in. Mayboroda, 36, is a mathematician at the University of Minnesota. Five years ago, she began to untangle the long-standing puzzle of localization. She came up with a mathematical formula called the “landscape function” that predicts exactly where waves will localize and what form they’ll take when they do.
“You want to know how to find these areas of localization,” Mayboroda said. “The naive approach is difficult. The landscape function magically gives a way of doing it.”
Her work began in the realm of pure mathematics, but unlike most mathematical advances, which might find a practical use after decades, if ever, her work is already being applied by physicists. In particular, LED lights — or light-emitting diodes — depend on the phenomenon of localization. They light up when electrons in a semiconducting material, having started out in a position of higher energy, get trapped (or “localize”) in a position of lower energy and emit the difference as a photon of light. LEDs are still a work in progress: Engineers need to build LEDs that more efficiently convert electrons into light, if the devices are to become the future of artificial lighting, as many expect they will. If physicists can gain a better understanding of the mathematics of localization, engineers can build better LEDs — and with the help of Mayboroda’s mathematics, that effort is already under way.
Rogue Waves
Localization is not an intuitive concept. Imagine you stood on one side of a room and watched someone ring a bell, only the sound never reached your ears. Now imagine that the reason it didn’t is that the sound had fallen into an architectural trap, like the sound of the sea bottled in a shell.
",Quanta Magazine,"Mathematicians, LED",12.3.13,11.2.14
14,"Write a hash table in C
","Hash tables are one of the most useful data structures. Their quick and scalable insert, search and delete make them relevant to a large number of computer science problems.
In this tutorial, we implement an open-addressed, double-hashed hash table in C. By working through this tutorial, you will gain:
	•	Understanding of how a fundamental data structure works under the hood
	•	Deeper knowledge of when to use hash tables, when not to use them, and how they can fail
	•	Exposure to new C code
C is a great language to write a hash table in because:
	•	The language doesn't come with one included
	•	It is a low-level language, so you get deeper exposure to how things work at a machine level
This tutorial assumes some familiarity with programming and C syntax. The code itself is relatively straightforward, and most issues should be solvable with a web search. If you run into further problems, please open a GitHub Issue.
The full implementation is around 200 lines of code, and should take around an hour or two to work through.

",Github,"Hash tables, C",12.12.12,11.11.15
15,"Google unveils a new, cheaper networking option for cloud customers: the public internet
","Google Cloud Platform customers will have a new option when selecting the type of network used to deliver their traffic to their users: they can keep using Google’s network, or they can save some money with the new option of using public transit networks.
Google has long argued that one of the best reasons to use its public cloud service is the strength of its fiber network, developed and enhanced for more than a decade to support the global data centers powering its search engine. But there are some applications that don’t require that level of performance, and so Google is now offering a cheaper networking service that uses the transit networks that deliver the bulk of traffic to internet service providers, said Prajakta Joshi, product manager for cloud networking at Google.
The new “Standard Tier” should offer performance comparable to what customers would experience through “other cloud providers,” Joshi said, although both Amazon Web Services and Microsoft Azure operate fiber networks outside of the public internet. A basic networking 101 reminder: most internet traffic passes through a number of different intersections from its source on the way to its destination, but Google’s “Premium Tier” network was built outside that system to support its global search ambitions by reducing the number of “hops” data is forced to take along the way.
",Geek Wire,Google,3.15.2016,3.17.2017
